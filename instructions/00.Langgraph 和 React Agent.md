# 一、LangGraph 的核心思想

LangGraph 是一个可以让开发者以**图（Graph）**的方式来编排对话式AI流程的库，提供了以下能力：

1. **状态驱动**：在传统的对话模型中，我们经常需要维护对话上下文、剩余步骤等各种内部变量。LangGraph 将这些变量统一到一个“状态(State)”里，并约定任何节点的输入/输出都以“状态(State)”的形式表示。
   
2. **可视化执行流**：LangGraph 将对话/工具调用/自定义逻辑封装成“节点(Node)”与“边(Edge)”。当图被编译后，执行流会在节点之间穿梭，处理对话消息、调用工具、终止或转向某些分支。

3. **可组合**：你可以把一个复杂的对话逻辑拆分为多个可复用的子图，每个子图都可以独立进行单元测试或复用在更大的图中。

4. **多步思考 + 工具调用**：通过 ReAct Agent（一个经典的多步推理+工具调用范式），LangGraph 可以帮助你自动管理**多次**调用语言模型及其衍生工具的过程——只要你把“工具”注册到图里。

在使用时，你基本会经历如下步骤：

1. **定义状态模式（state schema）**：说明 state 中必须包含哪些字段（如：对话消息 `messages`，剩余可用步骤 `remaining_steps`，等等）。
2. **定义节点（Node）**：比如一个负责调用LLM的节点、一个负责执行特定工具的节点、或者你自定义的Python逻辑节点。
3. **连接边（Edges）**：决定每个节点之后，下一步走到哪个节点；也可以做条件分支或循环。
4. **编译图（Compile）**：LangGraph 会把你的“编排逻辑”转换为一个 LangChain 兼容的“可调用对象(CompiledGraph)”。
5. **执行或流式执行**：可以直接一次性 `graph.invoke(...)` 得到最终结果，也可以使用 `graph.stream(...)` 流式获取每个“阶段性状态（partial state）” 。

---

# 二、ReAct Agent 与 create_react_agent 概念

## 2.1 什么是 ReAct Agent

“ReAct” 是一种典型的LLM多步推理与工具调用策略。它主要包含两部分：

1. **Reasoning**：先让语言模型（LLM）进行一步推理，产出一个潜在的思考过程以及可能的工具调用。
2. **Acting**：如果模型说“我要调用某个工具”，则执行该工具，得到结果，再把结果加入对话，然后让模型再次 Reason，看看是否还需要再调用工具，或输出最后的答案。

这个循环可以**多次往返**，直到模型不再调用工具，输出最终结果。

## 2.2 create_react_agent 做了什么

`create_react_agent(...)` 是 LangGraph 中的一个快捷方法，用于**快速创建**一个可执行的“ReAct风格”图（Graph）：

- **自动添加“agent节点”**：用来调用你的语言模型（并在对话中发出可能的工具调用）。
- **自动添加“tools节点”**：如果 agent 的输出中含有工具调用（tool_calls），则会交给 tools 节点逐个执行，并把执行结果以 `ToolMessage` 的形式返回到对话中。
- **自动在 agent ↔ tools 之间连线**：只要 agent 产生了工具调用，就进入 tools；tools 执行完返回消息后，再回到 agent；直到不再有工具调用为止。
- **可选 structured output**：如果你传入了 `response_format` 参数，LangGraph 会在最后一步生成一个结构化输出(“JSON Schema”、“Pydantic”、“OpenAI function schema”等)，以便你获取可解析的最终结果。
- **控制“剩余步骤”**：Agent 每次回答后会检查是否还可以继续调用工具，或者是否需要中止并返回错误（“抱歉，需要更多步骤”）。

因此，调用 `create_react_agent(...)` 得到的结果，是一个**已经配置好**的 “CompiledGraph”。这个图中带有 “agent” 节点（LLM） 和 “tools” 节点（调用工具），以及检查**是否还有工具要调**的逻辑。你可以直接拿这个对象执行，获得一个 ReAct 流程的多轮对话+工具使用。

---

# 三、create_react_agent 函数解析

让我们逐段看看你给出的源码要点：

```python
def create_react_agent(
    model: Union[str, LanguageModelLike],
    tools: Union[Sequence[BaseTool], ToolNode],
    *,
    prompt: Optional[Prompt] = None,
    response_format: Optional[
        Union[StructuredResponseSchema, tuple[str, StructuredResponseSchema]]
    ] = None,
    state_schema: Optional[StateSchemaType] = None,
    config_schema: Optional[Type[Any]] = None,
    checkpointer: Optional[Checkpointer] = None,
    store: Optional[BaseStore] = None,
    interrupt_before: Optional[list[str]] = None,
    interrupt_after: Optional[list[str]] = None,
    debug: bool = False,
    version: Literal["v1", "v2"] = "v1",
    name: Optional[str] = None,
) -> CompiledGraph:
    ...
    ...
```

### 3.1 主要参数介绍

1. **model**: 传入一个语言模型（LLM），支持  
   - `str` 形式(`"openai:gpt-3.5-turbo"`, `"anthropic:claude-v1"` 等等)，前提是安装了 `langchain` 并支持“自动init”  
   - 或直接传入一个符合 `LanguageModelLike` 协议的模型对象（比如 `ChatOpenAI`, `ChatAnthropic` 等）
2. **tools**: 一个工具列表(List[BaseTool])，或一个 `ToolNode`。  
   如果你只是传入一个列表，这个函数里会自动帮你生成 `ToolNode` 节点。工具节点的作用是——当语言模型请求调用工具时，将调用映射到你给定的 Python 函数或自定义工具对象上。
3. **prompt**: 额外的提示词。如果给了它，会在对话最前面插入这个“system prompt”，或者覆盖消息处理的逻辑（也支持 ChatPromptTemplate 或自定义 callables）。
4. **response_format**: 如果你需要**结构化输出**，可在这里提供一个 `StructuredResponseSchema` 或 `(str, StructuredResponseSchema)` 的 tuple；LangGraph 会在 Agent 完成对话循环后，再进行一次额外的 LLM 调用来生成符合此 schema 的输出（这属于“多次调用”中的最后一次专门生成结构化结果）。
5. **state_schema**: 自定义“图的状态”类型，必须包含 `messages` 和 `remaining_steps` 字段（如果要用多步 ReAct），并可在里头加你自己的额外字段。若不传，就用内置的 `AgentState` / `AgentStateWithStructuredResponse`。
6. **checkpointer**、**store**: 可以把状态和数据持久化到某种存储中（如内存、Redis、数据库等），从而在多会话或长会话中保留上下文。
7. **interrupt_before / interrupt_after**: 用于在执行某个节点（"agent" 或 "tools"）前或后，插入“打断逻辑”（例如让用户确认）。不是日志用途，而是对话流程插入交互的机制。
8. **debug**: 如果为 True，会输出一些更详细的执行信息。
9. **version**: "v1" or "v2"。  
   - v1 的做法：把一个 AIMessage 中的 tool_calls 全部一起执行完，然后回到 agent；  
   - v2 的做法：可以“一条一条”地发送 tool call，更灵活，但也更复杂。

### 3.2 核心构造逻辑

在 `create_react_agent` 里，按照下列顺序进行处理：

1. **兼容处理**：如果 `model` 是字符串，会调用 `init_chat_model` 去创建对应的 LLM 对象；否则直接使用你传入的 LLM。
2. **确定 tool_calling_enabled**：如果你传入了至少一个 tool，则启用“工具调用”流程；否则就只是一个单纯的 LLM Node。
3. **绑定工具**：若你的 LLM 具备 `.bind_tools([...])` 方法（类似 `ChatOpenAI.bind_tools(...)`），则会在内部把工具签名注入到 LLM 中，以便它能“自动生成”工具调用。
4. **构建 agent 节点**：主要是一个调用LLM的包装函数，每次都把 `messages` 传给模型，看它是否要调用工具。并在这里**检查剩余步骤**——如果没有足够的剩余步骤，但依然有工具调用，则直接返回一个错误AIMessage。
5. **构建 tools 节点**：如果你传入的是 `ToolNode` 就直接用；如果你传了 `[BaseTool, ...]` 列表，则生成一个 `ToolNode(tools=...)`。
6. **在图中添加 edge**：  
   - agent → tools: 如果 agent 生成了工具调用  
   - tools → agent: 当 tools 执行完毕，把结果插入到 `messages`，再回到 agent；  
   - 如果 agent 不再有 tool_calls，就结束(END) 或进入 “generate_structured_response” 节点。  
   - 当然，如果设置了**tool.return_direct**，则会直接返回，而不是再去调用 agent。
7. **可选：添加结构化输出节点**(`generate_structured_response`)：如果提供了 `response_format`，则 agent 结束后再单独调用一次 LLM 来“整理”最终回答到你指定的schema里。
8. **compile**：调用 `workflow.compile(...)` 得到最后的“可执行Graph”（`CompiledGraph`）。

当你拿到这个 `CompiledGraph` 之后，你就可以这样调用：

- `graph.invoke(inputs)`：一次性得到最终结果。（如果有多次工具调用，也会在内部自动循环。）
- `graph.stream(inputs, stream_mode="values")`：以迭代的形式返回**每一步**/ **每阶段**的“部分状态 (partial_state)”。

---

# 四、执行流程：从输入到输出

创建好 ReAct 图后，你给它一个输入状态（最少包含 `"messages"`，如 `{"messages": [("user", "Hello!")]}`）。执行过程大体是：

1. **entry point: "agent"**  
   进入 agent 节点，它会从 state["messages"] 中取出消息，交给 LLM 生成一个 AIMessage。如果 AIMessage 包含 tool_calls，那么 state 会更新多一些字段，比如 `messages` 后面多了这个 AIMessage。

2. **检查是否要调用工具**  
   - 如果 `tool_calls` 不为空，则顺着 edges 进入 "tools" 节点。
   - 如果没有 tool_calls，则表示 agent 没有想调用任何工具 -> 流程会判断是否要去 “generate_structured_response” 或 “END”。
   
3. **tools 节点执行**  
   "tools" 节点会去匹配 agent 要调用的工具，比如：  
   ```json
   {
     "name": "search_tool",
     "args": {"query": "something"},
     "id": "call_abc123"
   }
   ```
   然后运行相应的 Python 函数，得到结果后，包装成 `ToolMessage`，附加回 state["messages"] 列表里。
   - 如果 agent 一次性请求了多个工具，在 v1 版本中则会并行执行，再把返回结果依次追加到 messages 里。
   - 在 v2 版本中，LangGraph 会拆分 tool_calls 分批执行。

4. **回到 agent**  
   现在 agent 再次拿到新的 state["messages"]（多了“ToolMessage”），就会针对最新的对话上下文重新进行思考——是否要再调用别的工具、或者是否直接产出最终回答？

5. **循环，直到不再调用工具**  
   只要 AIMessage 继续发出 tool_calls，就进入 Tools 节点；Tools 执行完再回到 Agent 节点。这一过程可能多次往返。（如果你设定了 `remaining_steps`，LangGraph 在每一轮都会减少1，直到不足时终止或报错，避免死循环。）

6. **可选：结构化输出**  
   在最后如果 `response_format` 存在，图会跳到“generate_structured_response”节点，再次对(几乎)所有对话做一次 LLM 调用，要求 LLM 产出符合**你给定schema**的 JSON，并存入 `structured_response` 字段中。然后再返回 END。

7. **结束**  
   整个 ReAct 流程完成后，图会返回一个最终状态，如：
   ```python
   {
     "messages": [  # 所有对话消息(包含了Human/AI/Tools等),
       ...,
       AIMessage(content="Here is the final answer", tool_calls=[])
     ],
     "remaining_steps": 2,
     "structured_response": { ... }  # 如果使用了response_format
   }
   ```
   你可以从中拿到想要的最终 AI 回答。

---

# 五、如何查看“中间推理”或“工具调用”？

从 **langgraph 0.3** 开始，`create_react_agent` 及其返回的 Graph 已经**不再支持** `graph.add_state_change_listener` 或在函数参数里传入 `callbacks`。如果你想**监听**或**打印** Agent 的中间思考、工具调用等过程，最好的方式是 **使用 `graph.stream(...)`**——它会在每一小步执行结束后产出一个“部分状态( partial_state )”，你可以在循环里进行日志记录、可视化或其他操作。示例：

```python
graph = create_react_agent(model, tools=[...], prompt="...")

inputs = {
    "messages": [
       ("user", "请分析特斯拉2025年的发展预期，包括新车型计划、销量目标、技术创新和市场扩张战略。")
    ]
}

for partial_state in graph.stream(inputs, stream_mode="values"):
    messages = partial_state["messages"]
    last_msg = messages[-1]
    if last_msg.type == "ai":
        print("[AIMessage] => ", last_msg.content)
        if last_msg.tool_calls:
            print("AI wants to call tools:", last_msg.tool_calls)
    elif last_msg.type == "tool":
        print("[ToolMessage] => Name:", last_msg.name, "Content:", last_msg.content)
    elif last_msg.type == "human":
        print("[User] => ", last_msg.content)

# 最后一次迭代时，partial_state 就是最终结果
final_answer = partial_state["messages"][-1].content
print("最终回答:", final_answer)
```

这样就能够**在每一次** Agent 或 Tools 完成后都获取状态，不需要“回调监听器”。

---

# 六、关于一些进阶用法

1. **`interrupt_before` / `interrupt_after`**  
   如果你希望在“agent”节点**执行前**或者**后**打断，可以设置这两个可选参数，比如：
   ```python
   create_react_agent(
       model,
       tools=[...],
       interrupt_before=["tools"],
       interrupt_after=["agent"],
       ...
   )
   ```
   当执行流程跑到 agent 或 tools 时，会先/后给你一个“交互点”机会，你可以在**流式**执行中察觉到这个点，或者抛出异常提前终止等。但是它比较适合做“用户确认”或“调试介入”，而不是实时日志。

2. **`checkpointer` / `store`**  
   - `checkpointer` 主要用来将单个“线程”（单条对话）的状态进行保存、恢复，可以在多回合对话里保留上下文。
   - `store` 提供了更跨线程或跨用户的持久化能力。  
   通过把 `store` 绑定到 Graph，工具调用里还可以使用 `InjectedStore`，把数据写入或读取到 store 中（如相当于“全局数据库”）。

3. **`response_format`**  
   如果你想让最终输出符合某种 JSON Schema 或 Pydantic 验证，可以这样写：
   ```python
   from langchain_core.prompts import ChatPromptTemplate
   from pydantic import BaseModel, Field

   class TeslaPlan(BaseModel):
       new_models: list[str] = Field(..., description="新车型列表")
       sales_target: int = Field(..., description="预计销量")
       technology_innovations: str
       market_strategy: str

   my_response_format = TeslaPlan

   graph = create_react_agent(
       model, 
       tools, 
       prompt="你是一个专业汽车分析师。",
       response_format=my_response_format
   )
   ```
   当 ReAct 流程结束后，LangGraph 会调用一次 LLM 并要求它返回符合 `TeslaPlan` 的 JSON。最终的 `state["structured_response"]` 就是一个 Python 字典或 Pydantic 实例。

4. **`version="v1" / "v2"`**  
   - **v1**: 工具调用是“把当前 AIMessage 中的所有 tool_calls 一次性并行执行” → tools → 再回到 agent。  
   - **v2**: 更细粒度地把每个 tool_call 拆开，每个都进入一个独立的 ToolNode 实例。如果一个 AIMessage 里有 3 个 tool_calls，就会做 3 次独立的“tools执行→回到agent”循环**（通过 Send API）**。这种方式可以在多工具协作里更灵活，也可以插入更多自定义逻辑，但要做好相应的结构化处理。

---

# 七、常见问题与答疑

1. **Q: 我在旧版本使用 `graph.add_state_change_listener` 或 `callbacks`，现在为什么报错？**  
   A: 因为新的 LangGraph 0.3 取消了这种回调API，推荐使用 `graph.stream(...)` 在每一步迭代中自行处理日志或监听逻辑。

2. **Q: 如果不想每次都多轮循环，而只想 LLMC 接受一次输入就结束，怎么做？**  
   A: 你可以传递 `tools=[]`（空）到 `create_react_agent`，这样它就生成一个不支持工具调用的图；agent 只会输出一次，然后就结束。此时相当于纯LLM调用。

3. **Q: 要怎么限制调用工具的次数？**  
   A: 你可以在输入的 `state` 里设置 `remaining_steps`，或自定义 `AgentState` 包含 `remaining_steps=3` 一类初始值，每次 agent节点执行后，LangGraph 会自动减少1。用完就不会再允许工具调用了。

4. **Q: ReAct 会在同一个消息里多次请求调用工具吗？**  
   A: 是可能的。尤其是当 LLM 在回答中生成多个 tool_calls，就会全部执行。你可以在 `v1` 模式下并行运行它们，也可以在 `v2` 模式下逐个执行。

5. **Q: structured response 里的提示是如何工作的？**  
   A: 当 `response_format` 是 `(system_prompt, schema)` 这种 tuple 时，LangGraph 会在最后的 LLM 调用里给一个额外的 system_prompt，引导 LLM 返回符合 schema 的 JSON。这样可以做更严格的结构化要求。

---

# 八、总结

- **LangGraph** 是一个以“图”来编排对话和工具调用的框架；  
- **create_react_agent** 是“快捷构造 ReAct 风格图”的核心函数，一次性帮你搭建“agent(LLM) ↔ tools(工具节点) ↔ agent”循环；  
- 执行时默认从 `agent` 开始，如果 `AIMessage` 包含 `tool_calls` 就调用 `tools` 并注入结果，直到不再有工具调用；  
- 可以**流式**(`graph.stream(...)`) 或**一次性**(`graph.invoke(...)`)获取结果；  
- 要想查看中间推理和调用日志，使用 `stream` 在每一步循环里记录；  
- 可选地，你能通过 `interrupt_before` / `interrupt_after` 或 `checkpointer` / `store` 等更高级特性进一步定制执行流程或存储/恢复状态。

这就是从**原理**到**源码**再到**执行流程**的完整解析。希望能帮助你在实际项目里更好地运用 `create_react_agent` 和 LangGraph！